# -*- coding: utf-8 -*-
"""spam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rb57pIR5w8Xt1OOe1p_squujr60FL2FK
"""

import gdown
import chardet
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import f1_score
import torch.nn.functional as F
from transformers import BertTokenizer, BertForSequenceClassification

url = "https://drive.google.com/uc?id=1gg-OD4pQeyShfUt4w7QqxYQDOeS3trF0"
output = "spam.csv"
gdown.download(url,output)

with open("spam.csv", "rb") as file:
    result = chardet.detect(file.read())
    print(result)

data = pd.read_csv("spam.csv", encoding="Windows-1252")

data

data = data.drop(["Unnamed: 2", "Unnamed: 3", "Unnamed: 4"], axis=1)
X = data['v2']
y = data['v1']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a mapping for 'ham' and 'spam' to numerical values
label_mapping = {'ham': 0, 'spam': 1}

# Apply the mapping to convert y_train and y_test to integers
y_train = y_train.map(label_mapping)
y_test = y_test.map(label_mapping)

"""# LSTM"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def build_dict(texts):
    word_count = {}
    for text in texts:
        for word in text.split():
            word_count[word] = word_count.get(word, 0) + 1
    return {word: idx + 1 for idx, word in enumerate(word_count.keys())}

# Préparer les données pour LSTM
dictionary = build_dict(X_train)

# Calculate max_seq_len based on both train and test data
max_seq_len = max(
    max(len(x.split()) for x in X_train),  # Max length in training data
    max(len(x.split()) for x in X_test)   # Max length in test data
)

def tokenize_and_pad(texts, dictionary, max_seq_len):
    sequences = []
    for text in texts:
        seq = [dictionary.get(word, 0) for word in text.split()]
        # Ensure padding to match max_seq_len
        sequences.append(seq + [0] * (max_seq_len - len(seq)))
    return np.array(sequences)

# Tokenize and pad the datasets
X_train_lstm = tokenize_and_pad(X_train, dictionary, max_seq_len)
X_test_lstm = tokenize_and_pad(X_test, dictionary, max_seq_len)

# Convertir en tenseurs PyTorch
train_data = torch.utils.data.TensorDataset(
    torch.tensor(X_train_lstm, dtype=torch.long),
    torch.tensor(y_train.astype(int).values, dtype=torch.float32),  # Convert labels to int
)
test_data = torch.utils.data.TensorDataset(
    torch.tensor(X_test_lstm, dtype=torch.long),
    torch.tensor(y_test.astype(int).values, dtype=torch.float32),
)
train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
test_loader = DataLoader(test_data, batch_size=32, shuffle=False)

# Définir le modèle LSTM
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, hidden_dim, output_dim, n_layers, dropout):
        super(LSTMClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_dim)
        self.lstm = nn.LSTM(hidden_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        out = self.embedding(x)
        out, (hidden, cell) = self.lstm(out)
        out = self.fc(out[:, -1, :])  # Use the last output of LSTM
        out = self.sigmoid(out)
        return out.squeeze(-1)  # Ensure output is [batch_size]

# Initialisation du modèle
device = "cuda" if torch.cuda.is_available() else "cpu"
model = LSTMClassifier(len(dictionary) + 1, 128, 1, 1, 0.2).to(device)
criterion = nn.BCEWithLogitsLoss()  # BCEWithLogitsLoss est plus stable
optimizer = optim.Adam(model.parameters(), lr=0.0001)

for epoch in range(5):
    model.train()
    train_predictions = []
    train_true_labels = []

    for x_batch, y_batch in train_loader:
        x = x_batch.to(device)
        y = y_batch.to(device)

        optimizer.zero_grad()
        y_pred = model(x)
        loss = criterion(y_pred, y)
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient Clipping
        optimizer.step()

        train_predictions += list((torch.sigmoid(y_pred) >= 0.5).cpu().numpy())
        train_true_labels += list(y.cpu().numpy())

    # Évaluation
    model.eval()
    test_predictions = []
    test_true_labels = []
    with torch.no_grad():
        for x_batch, y_batch in test_loader:
            x = x_batch.to(device)
            y = y_batch.to(device)

            y_pred = model(x)
            test_predictions += list((torch.sigmoid(y_pred) >= 0.5).cpu().numpy())
            test_true_labels += list(y.cpu().numpy())

    train_f1 = f1_score(train_true_labels, train_predictions)
    test_f1 = f1_score(test_true_labels, test_predictions)

    print(f"epoch: {epoch+1}, loss: {loss.item():.5f}, train_f1: {train_f1:.5f}, test_f1: {test_f1:.5f}")

"""# BERT"""

from sklearn.metrics import f1_score, accuracy_score
from transformers import BertTokenizer, BertForSequenceClassification
import torch
from torch.utils.data import DataLoader
import torch.optim as optim

# Charger le tokenizer et préparer les données pour BERT
tokenizer = BertTokenizer.from_pretrained("nlpaueb/legal-bert-small-uncased")

def encode_texts(texts, tokenizer, max_len):
    return tokenizer(
        list(texts), max_length=max_len, padding="max_length", truncation=True, return_tensors="pt"
    )

train_encodings = encode_texts(X_train, tokenizer, 64)
test_encodings = encode_texts(X_test, tokenizer, 64)

train_data = torch.utils.data.TensorDataset(
    train_encodings["input_ids"], train_encodings["attention_mask"], torch.tensor(y_train.values, dtype=torch.float32)
)
test_data = torch.utils.data.TensorDataset(
    test_encodings["input_ids"], test_encodings["attention_mask"], torch.tensor(y_test.values, dtype=torch.float32)
)

train_loader = DataLoader(train_data, batch_size=16, shuffle=True)
test_loader = DataLoader(test_data, batch_size=16, shuffle=False)

# Charger le modèle BERT
bert_model = BertForSequenceClassification.from_pretrained("nlpaueb/legal-bert-small-uncased", num_labels=1).to("cuda")
optimizer = optim.Adam(bert_model.parameters(), lr=2e-5)

# Entraînement
for epoch in range(3):  # Ajustez le nombre d'époques
    bert_model.train()
    for batch in train_loader:
        input_ids, attention_mask, labels = [b.to("cuda") for b in batch]
        optimizer.zero_grad()
        outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

# Évaluation
bert_model.eval()
y_pred_bert = []
y_true_bert = []

with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [b.to("cuda") for b in batch]
        outputs = bert_model(input_ids, attention_mask=attention_mask)

        # Appliquer la sigmoid sur les logits pour obtenir des probabilités
        predictions = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()

        # Convertir les probabilités en classes binaires (0 ou 1)
        y_pred_bert.extend(predictions > 0.5)  # Si la probabilité > 0.5, prédire 1, sinon 0
        y_true_bert.extend(labels.cpu().numpy())

# Calcul des métriques
test_f1 = f1_score(y_true_bert, y_pred_bert)
test_accuracy = accuracy_score(y_true_bert, y_pred_bert)

print("BERT Accuracy:", test_accuracy)
print("BERT F1-Score:", test_f1)

